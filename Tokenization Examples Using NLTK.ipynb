{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a042244d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/dev/anaconda3/lib/python3.11/site-packages (3.7)\n",
      "Requirement already satisfied: click in /home/dev/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/dev/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/dev/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /home/dev/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e928c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dev/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78305a4d",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b817012",
   "metadata": {},
   "outputs": [],
   "source": [
    "## paragraphs ---> sentence\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ae3b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Tokenization in natural language processing (NLP) is a technique that involves dividing a sentence or phrase into smaller units known as tokens. These tokens can encompass words, dates, punctuation marks, or even fragments of words. The article aims to cover the fundamentals of tokenization, it’s types and use case.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb3bf253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization in natural language processing (NLP) is a technique that involves dividing a sentence or phrase into smaller units known as tokens. These tokens can encompass words, dates, punctuation marks, or even fragments of words. The article aims to cover the fundamentals of tokenization, it’s types and use case.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1361617e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization in natural language processing (NLP) is a technique that involves dividing a sentence or phrase into smaller units known as tokens.',\n",
       " 'These tokens can encompass words, dates, punctuation marks, or even fragments of words.',\n",
       " 'The article aims to cover the fundamentals of tokenization, it’s types and use case.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = sent_tokenize(corpus, language=\"english\")\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e725d7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'technique',\n",
       " 'that',\n",
       " 'involves',\n",
       " 'dividing',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'or',\n",
       " 'phrase',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'units',\n",
       " 'known',\n",
       " 'as',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'These',\n",
       " 'tokens',\n",
       " 'can',\n",
       " 'encompass',\n",
       " 'words',\n",
       " ',',\n",
       " 'dates',\n",
       " ',',\n",
       " 'punctuation',\n",
       " 'marks',\n",
       " ',',\n",
       " 'or',\n",
       " 'even',\n",
       " 'fragments',\n",
       " 'of',\n",
       " 'words',\n",
       " '.',\n",
       " 'The',\n",
       " 'article',\n",
       " 'aims',\n",
       " 'to',\n",
       " 'cover',\n",
       " 'the',\n",
       " 'fundamentals',\n",
       " 'of',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'it',\n",
       " '’',\n",
       " 's',\n",
       " 'types',\n",
       " 'and',\n",
       " 'use',\n",
       " 'case',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## paragraph ---> words\n",
    "## sentence ---> words\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(corpus)\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fac21fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'technique', 'that', 'involves', 'dividing', 'a', 'sentence', 'or', 'phrase', 'into', 'smaller', 'units', 'known', 'as', 'tokens', '.']\n",
      "['These', 'tokens', 'can', 'encompass', 'words', ',', 'dates', ',', 'punctuation', 'marks', ',', 'or', 'even', 'fragments', 'of', 'words', '.']\n",
      "['The', 'article', 'aims', 'to', 'cover', 'the', 'fundamentals', 'of', 'tokenization', ',', 'it', '’', 's', 'types', 'and', 'use', 'case', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70b764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a164723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b6cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73be6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
